# -*- coding: utf-8 -*-
"""Model_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hc_J_3cRTH7QB3O9c7yNZIH3yeK_RtwC
"""

!pip install transformers
!pip install ijson

# Install OpenAI's CLIP model via GitHub ‚Äî needed for extracting image embeddings using the CLIP model
!pip install git+https://github.com/openai/CLIP.git

import os
import random
import pickle

import pandas as pd
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from transformers import (
    AutoTokenizer,
    RobertaModel,
    Trainer,
    TrainingArguments,
    TrainerCallback,
    EarlyStoppingCallback
)

from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, f1_score, confusion_matrix

import matplotlib.pyplot as plt
import seaborn as sns

from tqdm import tqdm

import clip
from PIL import Image

from google.colab import drive
drive.mount('/content/drive')
data_dir = "/content/drive/MyDrive/TwiBot-22/TwiBot-22/"
load_dir = "/content/drive/MyDrive/TwiBot-22/prepared_data"

# Load saved data that was preprocessed earlier

# Load the list of user IDs selected for training/testing (sampled 50k‚Äì50k from each class)
with open(os.path.join(load_dir, "sampled_ids.pkl"), "rb") as f:
    sampled_ids = pickle.load(f)

# Load the user metadata as a pandas DataFrame (contains usernames, descriptions, numeric stats, etc.)
user_df = pd.read_pickle(os.path.join(load_dir, "user_df.pkl"))

# Load the dictionary mapping user IDs to ground-truth labels (0 = human, 1 = bot)
with open(os.path.join(load_dir, "label_dict.pkl"), "rb") as f:
    label_dict = pickle.load(f)

# Load the dictionary mapping user IDs to their cleaned tweet lists
with open(os.path.join(load_dir, "user_tweets.pkl"), "rb") as f:
    user_tweets = pickle.load(f)

# Load the list of sampled bot user IDs (after filtering + random sampling)
with open(os.path.join(load_dir, "sampled_bot_ids.pkl"), "rb") as f:
    sampled_bot_ids = pickle.load(f)

# Load the list of sampled human user IDs (after filtering + random sampling)
with open(os.path.join(load_dir, "sampled_human_ids.pkl"), "rb") as f:
    sampled_human_ids = pickle.load(f)

# Confirmation message
print("Loaded saved user data.")

# Load CLIP model and preprocessing pipeline (ViT-B/32 = Vision Transformer Base, patch size 32x32)
device = "cuda" if torch.cuda.is_available() else "cpu"

model_clip, preprocess_clip = clip.load("ViT-B/32", device=device)

image_dir = "/content/drive/MyDrive/TwiBot-22/Images"
clip_features = {}

# Loop over each user in the dataset (user_df)
for idx, row in tqdm(user_df.iterrows(), total=len(user_df), desc="Extracting CLIP features"):

    user_id = row["id"]
    subfolder = f"{(idx // 10000) * 10000:06d}-{(idx // 10000) * 10000 + 9999:06d}"
    img_path = os.path.join(image_dir, subfolder, f"{idx}.jpg")

    # If the image file exists for this user, try to process it
    if os.path.exists(img_path):
        try:
            # Load the image, resize it to 224x224, normalize pixel values (turns the image into a PyTorch tensor of shape [3, 224, 224]) and add a batch dimension ‚Üí shape becomes (1, 3, 224, 224)
            image = preprocess_clip(Image.open(img_path)).unsqueeze(0).to(device)

            # Pass the image through CLIP's vision encoder (ViT-B/32) to get a semantic feature vector
            with torch.no_grad():  # Disable gradients for speed and memory
                image_features = model_clip.encode_image(image).squeeze(0).cpu()  # Output shape: (512d)

            # Store the 512-dimensional vector in a dictionary under this user's ID
            clip_features[user_id] = image_features

        except:
            # Skip any image that is corrupted or cannot be opened
            pass

# After processing all users, save the dictionary to a .pt file
torch.save(clip_features, "/content/drive/MyDrive/TwiBot-22/clip_features.pt")

# This builds a dictionary like this:

# clip_features = {
#    "user1": tensor([...512 float values...]),
#    "user2": tensor([...512 float values...]),
#    ...
# }
# Each tensor represents the semantic features of the user‚Äôs profile image as computed by CLIP => and save this dictionary in clip_features.pt
# And Later in Training...
# load that file and use:
# clip_feature = self.clip_features.get(uid, torch.zeros(512))
# To get the corresponding image feature vector for each user. If it doesn't exist, we use a learnable embedding.

from google.colab import drive
drive.mount('/content/drive')

# Manually create train/val/test split with 70/15/15 proportions per class (bot and human), with a fixed random_state.

# Custom 70/15/15 balanced split
from sklearn.model_selection import train_test_split

# Step 1: Split bots: -----------------------------------------------------------------------------------------
# bot_train: 70% of the sampled bots ‚Äî for training
# bot_temp: 30% of the sampled bots ‚Äî a temporary set that will later be split into validation and test sets.
bot_train, bot_temp = train_test_split(sampled_bot_ids, test_size=0.30, random_state=42)

# splitting the 30% bot_temp set into two equal parts:
# bot_val: 15% of original bots ‚Äî for validation.
# bot_test: 15% of original bots ‚Äî for testing.
bot_val, bot_test = train_test_split(bot_temp, test_size=0.5, random_state=42)

# Step 2: Split humans:same strategy for a balanced dataset ---------------------------------------------------
human_train, human_temp = train_test_split(sampled_human_ids, test_size=0.30, random_state=42)
human_val, human_test = train_test_split(human_temp, test_size=0.5, random_state=42)

# Step 3: Combine bot + human IDs for each split --------------------------------------------------------------
train_ids = bot_train + human_train
val_ids = bot_val + human_val
test_ids = bot_test + human_test

# Print sizes of each split
print(f"‚úÖ Train IDs: {len(train_ids)}")
print(f"‚úÖ Val IDs: {len(val_ids)}")
print(f"‚úÖ Test IDs: {len(test_ids)}")

# Generate split_dict from your split
split_dict = {}
for uid in train_ids:
    split_dict[uid] = "train"
for uid in val_ids:
    split_dict[uid] = "val"
for uid in test_ids:
    split_dict[uid] = "test"

# Final check: Class distribution
from collections import Counter
print("üß™ Train label counts:", Counter([label_dict[uid] for uid in train_ids]))
print("üß™ Val label counts:", Counter([label_dict[uid] for uid in val_ids]))
print("üß™ Test label counts:", Counter([label_dict[uid] for uid in test_ids]))

# Initialize tokenizer
# This loads the pre-trained RoBERTa tokenizer from HuggingFace's model hub.
tokenizer = AutoTokenizer.from_pretrained("roberta-base")

# Custom PyTorch Dataset to load and prepare TwiBot user data for training
# For each user, it prepare textual data (username, description, and tweets) + numeric behavioral features (like follower-following ratio) + image features (precomputed CLIP embeddings of profile images)

class TwiBotDataset(Dataset):
    def __init__(self, user_ids, user_df, label_dict, user_tweets, clip_feature_path=None):
        self.user_ids = user_ids
        self.user_df = user_df.set_index('id')
        self.label_dict = label_dict
        self.user_tweets = user_tweets

        # Load precomputed CLIP features from file if path is given, else empty dict
        self.clip_features = torch.load(clip_feature_path) if clip_feature_path else {}

    def __len__(self):
        return len(self.user_ids)

    def __getitem__(self, idx):
        uid = self.user_ids[idx]

        # Sanity check: raise error if ID is missing from metadata or labels
        if uid not in self.user_df.index or uid not in self.label_dict:
            raise ValueError(f"Invalid user ID {uid}")

        # Get user metadata (username, description, numeric info)
        user_info = self.user_df.loc[uid]

        # Get list of tweets for this user (or empty list if missing)
        tweets = self.user_tweets.get(uid, [])

        # Combine profile-level text: username + description + tweets
        profile_text = user_info.get("username", "") + " " + user_info.get("description", "")
        tweets_text = " ".join(tweets)
        combined_text = profile_text + " " + tweets_text

        # Tokenize the full user text input for RoBERTa
        inputs = tokenizer(
            combined_text,
            truncation=True,
            padding='max_length',
            max_length=512,
            return_tensors="pt"
        )

        # Assign label (0 = human, 1 = bot)
        label = 1 if self.label_dict[uid] == "bot" else 0

        # Prepare numeric features (behavioral stats)
        numeric_features = torch.tensor([
            user_info.get('follower_following_ratio', 0.0),
            user_info.get('log_followers', 0.0),
            user_info.get('log_following', 0.0),
            user_info.get('log_tweet_count', 0.0)
        ], dtype=torch.float)

        # Load CLIP image feature for this user or fallback to zero vector if missing
        clip_feature = self.clip_features.get(uid, torch.zeros(512))

        # Return all tensors needed by the model
        return {
            'input_ids': inputs['input_ids'].squeeze(),             # Shape: (512,)
            'attention_mask': inputs['attention_mask'].squeeze(),   # Shape: (512,)
            'numeric_features': numeric_features,                   # Shape: (4,)
            'clip_features': clip_feature,                          # Shape: (512,)
            'labels': torch.tensor(label, dtype=torch.long)         # 0 or 1
        }

# ----------------------------------------------------------------------------------------

# Step 1: Precompute list of valid users (appear in both user_df and label_dict)
valid_user_ids = set(user_df['id']) & set(label_dict.keys())

# Step 2: Filter train/val/test splits to only include valid user IDs
valid_train_ids = [uid for uid in train_ids if uid in valid_user_ids]
valid_val_ids = [uid for uid in val_ids if uid in valid_user_ids]
valid_test_ids = [uid for uid in test_ids if uid in valid_user_ids]

# Step 3: Print stats about how many valid users were found
print(f"‚úÖ Valid training IDs: {len(valid_train_ids)} / {len(train_ids)}")
print(f"‚úÖ Valid validation IDs: {len(valid_val_ids)} / {len(val_ids)}")
print(f"‚úÖ Valid test IDs: {len(valid_test_ids)} / {len(test_ids)}")

# Step 4: Create dataset objects for each split
clip_path = "/content/drive/MyDrive/TwiBot-22/clip_features.pt"  # path to saved CLIP features
train_dataset = TwiBotDataset(valid_train_ids, user_df, label_dict, user_tweets, clip_feature_path=clip_path)
val_dataset = TwiBotDataset(valid_val_ids, user_df, label_dict, user_tweets, clip_feature_path=clip_path)
test_dataset = TwiBotDataset(valid_test_ids, user_df, label_dict, user_tweets, clip_feature_path=clip_path)

# Compute dynamic class weights from training labels
train_labels = [sample['labels'].item() for sample in train_dataset]
class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=train_labels)
class_weights = torch.tensor(class_weights, dtype=torch.float)
print("Dynamic class weights:", class_weights)

# Define the full multimodal model combining text, numeric, and image features

class RobertaWithNumericalFeaturesAndCLIP(nn.Module):
    def __init__(self, roberta_model_name='roberta-base', num_numeric_features=4, clip_feature_size=512):
        super().__init__()

        # Load pre-trained RoBERTa model to extract text features
        self.roberta = RobertaModel.from_pretrained(roberta_model_name)

        # Define a learnable fallback vector for users with missing CLIP image
        self.missing_clip_embedding = nn.Parameter(torch.zeros(clip_feature_size))  # Shape: (512,)
        nn.init.normal_(self.missing_clip_embedding, mean=0.0, std=0.02)  # Initialize it with small random values

        # Define a small neural network to compress RoBERTa output (768) to 256
        self.text_proj = nn.Sequential(
            nn.Linear(self.roberta.config.hidden_size, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # Define a projection for 4 numeric features to 32-dim vector
        self.numeric_proj = nn.Sequential(
            nn.Linear(num_numeric_features, 32),
            nn.LayerNorm(32),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # Define a projection for CLIP image features (512) to 128-dim vector
        self.clip_proj = nn.Sequential(
            nn.Linear(clip_feature_size, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # Define fusion layers to combine all features (256+32+128=416 ‚Üí 128)
        self.fusion = nn.Sequential(
            nn.Dropout(0.3),                  # Helps generalization
            nn.Linear(256 + 32 + 128, 256),   # First fusion layer
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),              # Reduce to final feature vector
            nn.ReLU()
        )

        # Final classification layer: 128 ‚Üí 2 (human or bot)
        self.out = nn.Linear(128, 2)

    def forward(self, input_ids, attention_mask, numeric_features, clip_features, labels=None):
        # Ensure all inputs are on the same device (CPU or GPU)
        numeric_features = numeric_features.to(input_ids.device)
        clip_features = clip_features.to(input_ids.device)

        # Replace CLIP vectors that are all zeros with the learnable missing image embedding
        mask_missing = torch.all(clip_features == 0, dim=1)  # Detect which vectors are all-zero
        missing_embed = self.missing_clip_embedding.unsqueeze(0).repeat(clip_features.size(0), 1).to(input_ids.device)
        clip_features = torch.where(mask_missing.unsqueeze(1), missing_embed, clip_features)

        # Pass text through RoBERTa and extract the CLS token output (summary vector)
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]  # Extract first token (<s>) from each input sequence

        # Project each modality separately
        text_feat = self.text_proj(cls_output)             # Shape: (batch_size, 256)
        num_feat = self.numeric_proj(numeric_features)     # Shape: (batch_size, 32)
        clip_feat = self.clip_proj(clip_features)          # Shape: (batch_size, 128)

        # Concatenate all three features into one vector
        combined = torch.cat((text_feat, num_feat, clip_feat), dim=1)  # Shape: (batch_size, 416)

        # Fuse and reduce dimensions via fusion layers
        x = self.fusion(combined)  # Shape: (batch_size, 128)

        # Final classification to produce logits for [human, bot]
        logits = self.out(x)       # Shape: (batch_size, 2)

        # If labels are provided (training), compute cross-entropy loss
        if labels is not None:
            loss = nn.functional.cross_entropy(logits, labels)
            return {'loss': loss, 'logits': logits}

        # If labels not provided (inference), return logits only
        return {'logits': logits}

# Instantiate the model
model = RobertaWithNumericalFeaturesAndCLIP()

# Freeze RoBERTa's weights initially (only train other parts of the model first)
for param in model.roberta.parameters():
    param.requires_grad = False

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",                # Directory to save model checkpoints and results
    # evaluation_strategy="epoch",         # [DEPRECATED] Mistyped version ‚Äî should be removed
    eval_strategy="epoch",                 # Evaluate the model at the end of each epoch
    learning_rate=3e-6,                    # Initial learning rate for the AdamW optimizer
    per_device_train_batch_size=8,         # Number of samples per GPU during training =>
                                                # Batch size = number of training samples processed at once before updating model weights.
                                                # Smaller batch = less GPU memory usage, but noisier gradients.
                                                # Larger batch = more stable gradients, but needs more RAM.
    per_device_eval_batch_size=8,          # Number of samples per GPU during evaluation
    num_train_epochs=7,                    # Number of full passes through the dataset => Later, use 3‚Äì5 epochs with the full dataset for final results.
                                                # An epoch is one full pass through the entire training dataset.
                                                # If you have 20,000 users in your dataset:
                                                # 1 epoch means the model sees all 20,000 once.
                                                # 2 epochs means the model sees them twice, learning more each time.
    warmup_ratio=0.1,                      # Helps stabilize training in early steps
    weight_decay=0.01,                     # Weight decay (L2 regularization) to prevent overfitting
    save_strategy="epoch",                 # Save model checkpoint at the end of every epoch
    logging_dir="./logs",                  # Directory for storing training logs (for TensorBoard or manual inspection)
    fp16=False,                            # enable if GPU supports it => The model uses 16-bit math where it's safe => and keeps 32-bit where needed (like in gradient updates) to preserve stability.
    logging_steps=10,                      # Log training loss etc. every 10 steps
    optim="adamw_torch",                   # Optional: Avoid fused optimizers that don't work on XLA
    report_to="none",                      # üö´ disables wandb and all other reporters => # Disabling Weights & Biases (wandb. It‚Äôs only used for logging metrics and plots to the cloud.

    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    save_total_limit=2,

)

# Custom Focal Loss implementation to focus more on hard examples => not used in the final model
class FocalLoss(nn.Module):
    def __init__(self, alpha=1.0, gamma=2.0, weight=None):
        super().__init__()
        self.alpha = alpha          # Scaling factor for the entire loss (optional tuning param)
        self.gamma = gamma          # Focusing parameter: higher gamma = more focus on hard-to-classify samples
        self.weight = weight        # Optional class weights for class imbalance (e.g., more weight to bots)

    def forward(self, logits, targets):
        # Compute standard cross-entropy loss per example without reducing (no mean yet)
        ce_loss = nn.functional.cross_entropy(logits, targets, weight=self.weight, reduction='none')
        # Convert cross-entropy loss to probability of correct class (p_t)
        pt = torch.exp(-ce_loss)
        # Apply focal loss formula: scale loss down for easy examples (high p_t), up for hard ones (low p_t)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()  # Return average loss across batch

# Extend HuggingFace Trainer to support class weighting and optional custom loss
class WeightedTrainer(Trainer):
    def __init__(self, *args, class_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights  # Store optional class weights for imbalanced labels

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        # Extract true labels from batch
        labels = inputs["labels"]
        # Run forward pass to get logits
        outputs = model(**inputs)
        logits = outputs["logits"]

        # Prepare class weight tensor if provided
        weight_tensor = self.class_weights.to(logits.device) if self.class_weights is not None else None
        # Compute weighted cross-entropy loss with label smoothing for better generalization
        loss_fn = nn.CrossEntropyLoss(weight=weight_tensor, label_smoothing=0.05)  # üÜï label smoothing prevents overconfidence
        loss = loss_fn(logits, labels)

        return (loss, outputs) if return_outputs else loss  # Return loss (and outputs if required)

# Define how to batch and stack tensors during training
def collate_fn(batch):
    # Filter out any corrupted or incomplete examples that are missing required keys
    clean_batch = [s for s in batch if s is not None and all(k in s for k in ['input_ids', 'attention_mask', 'numeric_features', 'clip_features', 'labels'])]

    # Stack tensors across the batch dimension to form a mini-batch
    return {
        'input_ids': torch.stack([s['input_ids'] for s in clean_batch]),                 # Shape: [batch_size, seq_len]
        'attention_mask': torch.stack([s['attention_mask'] for s in clean_batch]),       # Shape: [batch_size, seq_len]
        'numeric_features': torch.stack([s['numeric_features'] for s in clean_batch]),   # Shape: [batch_size, num_features]
        'clip_features': torch.stack([s['clip_features'] for s in clean_batch]),         # Shape: [batch_size, clip_dim]
        'labels': torch.tensor([s['labels'] for s in clean_batch])                       # Shape: [batch_size]
    }

# Simple accuracy metric used during evaluation
def compute_metrics(pred):
    labels = pred.label_ids                         # Ground truth labels
    preds = pred.predictions.argmax(-1)             # Predicted class (argmax over logits)
    acc = (preds == labels).mean()                  # Accuracy = (correct predictions / total)
    return {"accuracy": acc}

# Custom callback to unfreeze RoBERTa encoder after the first epoch
class UnfreezeRobertaCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        # If we‚Äôve completed at least one epoch, unfreeze RoBERTa layers to allow fine-tuning
        if state.epoch >= 1 and hasattr(kwargs['model'], 'roberta'):
            print("Unfreezing RoBERTa encoder after epoch 1...")
            for param in kwargs['model'].roberta.parameters():
                param.requires_grad = True  # Enable gradient updates for RoBERTa

# Initialize the custom Trainer with weighted loss and unfreeze callback
trainer = WeightedTrainer(
    model=model,                                       # Our multimodal model (text + numeric + image)
    args=training_args,                                # Training configuration (epochs, batch size, learning rate, etc.)
    train_dataset=train_dataset,                       # Training dataset
    eval_dataset=val_dataset,                          # Validation dataset
    compute_metrics=compute_metrics,                   # Metric function to evaluate during training
    data_collator=collate_fn,                          # Custom batch collation logic for multimodal input
    class_weights=class_weights,                       # Optional class weights to address label imbalance
    callbacks=[                                        # List of training callbacks
        UnfreezeRobertaCallback(),                     # Unfreeze RoBERTa encoder after epoch 1
        EarlyStoppingCallback(early_stopping_patience=2)  # Stop early if val loss doesn‚Äôt improve for 2 evals
    ],
)

print("üîç Checking a single item from train_dataset...")
sample = train_dataset[0]
print("‚úÖ Keys in sample:", sample.keys())

# Check types and shapes
print("‚úÖ input_ids shape:", sample['input_ids'].shape)
print("‚úÖ attention_mask shape:", sample['attention_mask'].shape)
print("‚úÖ numeric_features:", sample['numeric_features'])
print("‚úÖ labels:", sample['labels'])

print("\nüîç Checking a mini-batch from DataLoader...")
from torch.utils.data import DataLoader

batch_loader = DataLoader(train_dataset, batch_size=4, collate_fn=collate_fn)
batch = next(iter(batch_loader))

print("‚úÖ Batch keys:", batch.keys())
print("‚úÖ input_ids batch shape:", batch['input_ids'].shape)
print("‚úÖ attention_mask batch shape:", batch['attention_mask'].shape)
print("‚úÖ numeric_features batch shape:", batch['numeric_features'].shape)
print("‚úÖ labels batch shape:", batch['labels'].shape)

# Check for NaNs or invalid values in numeric features
import torch
if torch.isnan(batch['numeric_features']).any():
    print("‚ùå NaNs detected in numeric features!")
else:
    print("‚úÖ No NaNs in numeric features.")

# Check if model forward works
print("\nüîç Checking model forward pass...")

# ‚úÖ Move batch to the same device as model
device = next(model.parameters()).device
batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}

try:
    output = model(
        input_ids=batch['input_ids'],
        attention_mask=batch['attention_mask'],
        numeric_features=batch['numeric_features'],
        clip_features=batch['clip_features'],  # <-- this was missing
        #has_clip=batch['has_clip'],
        labels=batch['labels']  # ‚úÖ Fixed comma
    )
    print("‚úÖ Model output shape:", output['logits'].shape)
    print("‚úÖ Loss:", output['loss'].item())
    print("‚úÖ Logits shape:", output['logits'].shape)
except Exception as e:
    print("‚ùå Model forward pass failed:", str(e))

# Train the model
trainer.train()

import matplotlib.pyplot as plt

# Rebuild per-epoch metrics from logs
train_epochs = []
train_losses = []

val_epochs = []
val_losses = []
val_accs = []

for entry in trainer.state.log_history:
    if "loss" in entry and "epoch" in entry:
        train_epochs.append(entry["epoch"])
        train_losses.append(entry["loss"])
    if "eval_loss" in entry:
        val_epochs.append(entry["epoch"])
        val_losses.append(entry["eval_loss"])
    if "eval_accuracy" in entry:
        val_accs.append(entry["eval_accuracy"])

# Plot
plt.figure(figsize=(10, 4))

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(train_epochs, train_losses, label="Train Loss")
plt.plot(val_epochs, val_losses, label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training & Validation Loss")
plt.legend()

# Accuracy plot
plt.subplot(1, 2, 2)
plt.plot(val_epochs, val_accs, label="Val Accuracy", color='green')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Validation Accuracy")
plt.legend()

plt.tight_layout()
plt.show()

user_df_ids = set(user_df["id"].tolist())
user_tweet_ids = set(user_tweets.keys())

overlap = user_df_ids.intersection(user_tweet_ids)
print(f"Number of user IDs in both user_df and user_tweets: {len(overlap)}")

sample = train_dataset[0]
print(sample.keys())

print(train_dataset[0])

print(val_dataset[0])

# Evaluate on test set
test_results = trainer.evaluate(test_dataset)

print("\nTest Evaluation Results:")
for key, value in test_results.items():
    print(f"{key}: {value}")

from sklearn.metrics import classification_report, f1_score, confusion_matrix, precision_recall_fscore_support
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Predictions and true labels
predictions = trainer.predict(test_dataset)
logits = predictions.predictions
true_labels = predictions.label_ids

# Default threshold (argmax)
pred_labels = np.argmax(logits, axis=-1)

print("\nüîç Classification Report (Default Threshold = 0.5):")
print(classification_report(
    true_labels,
    pred_labels,
    labels=[0, 1],
    target_names=["human", "bot"],
    zero_division=0
))

# Macro and weighted F1 scores
macro_f1 = f1_score(true_labels, pred_labels, average='macro')
weighted_f1 = f1_score(true_labels, pred_labels, average='weighted')
print(f"\nMacro F1 Score: {macro_f1:.4f}")
print(f"Weighted F1 Score: {weighted_f1:.4f}")

# Confusion Matrix
cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["human", "bot"],
            yticklabels=["human", "bot"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix (Threshold = 0.5)")
plt.tight_layout()
plt.show()

# üîß Threshold tuning
print("\nüîß Threshold tuning results (based on softmax bot probability):")
probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1)[:, 1]  # prob of bot
thresholds = [0.3, 0.4, 0.45, 0.5, 0.55, 0.6]

for t in thresholds:
    pred_thresh = (probs > t).long().numpy()
    print(f"\nThreshold = {t}")
    print(classification_report(
        true_labels,
        pred_thresh,
        labels=[0, 1],
        target_names=["human", "bot"],
        zero_division=0
    ))

label_df = pd.read_csv(os.path.join(data_dir, "label.csv"))
label_counts = label_df['label'].value_counts()
print(label_counts)

labeled_ids = set(label_dict.keys())
split_train_ids = {uid for uid, split in split_dict.items() if split == "train"}
tweet_user_ids = set(user_tweets.keys())
user_ids_loaded = set(user_df["id"])

# Check overlap
final_train_pool = user_ids_loaded & labeled_ids & split_train_ids & tweet_user_ids
print(f"Final trainable user pool size: {len(final_train_pool)}")

# Check bot/human balance
bot_count = sum(1 for uid in final_train_pool if label_dict[uid] == "bot")
human_count = sum(1 for uid in final_train_pool if label_dict[uid] == "human")
print(f"Trainable pool - Bots: {bot_count}, Humans: {human_count}")